{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BibIndex - Auctoritates - colaboratory.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/tomaham/datasharing/blob/master/BibIndex_Auctoritates_colaboratory.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ehXeT6Olc3mJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Auctoricates BibIndex script v1.2 (Colaboratory version)\n",
        "###################################################################\n",
        "\n",
        "#0] authenticate google user\n",
        "#1] read google sheets from input params\n",
        "#2] do the stuff\n",
        "#3] prompt browser download dialog with result excel dataset\n",
        "#4] generate html output\n",
        "#5] prompt browser download dialog with result html file\n",
        "\n",
        "\n",
        "#todoos\n",
        "# validation of data? \n",
        "# warning for rows with error or non-standard values?\n",
        "# better parametrisation of sorting?\n",
        "\n",
        "#notes\n",
        "#https://github.com/davisd/python-scriptures, http://www.davisd.com/python-scriptures/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o9s1Zjufr_Kc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0sR06CHyc_Cm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "194a96e8-9423-4fbd-8758-bfaeadca7f0f"
      },
      "cell_type": "code",
      "source": [
        "!pip install gspread\n",
        "!pip install openpyxl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gspread\n",
            "  Downloading https://files.pythonhosted.org/packages/90/1f/7f919ef5c38ecd64c43fdb8b1b481c14e5651c13d07b2bf5421a7345e119/gspread-3.0.1.tar.gz\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python2.7/dist-packages (from gspread) (2.18.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.2.1->gspread) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.2.1->gspread) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.2.1->gspread) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.2.1->gspread) (3.0.4)\n",
            "Building wheels for collected packages: gspread\n",
            "  Running setup.py bdist_wheel for gspread ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/70/d4/87/c3e549f7548178be1cfb0f46f776919b979fdac98a9d270b5a\n",
            "Successfully built gspread\n",
            "Installing collected packages: gspread\n",
            "Successfully installed gspread-3.0.1\n",
            "Collecting openpyxl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/dd/5952829956827de7ff36eb70877fdffd6dbfacb670fae05eb7ccba52ace7/openpyxl-2.5.5.tar.gz (171kB)\n",
            "\u001b[K    100% |████████████████████████████████| 174kB 6.1MB/s \n",
            "\u001b[?25hCollecting jdcal (from openpyxl)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/38/dcf83532480f25284f3ef13f8ed63e03c58a65c9d3ba2a6a894ed9497207/jdcal-1.4-py2.py3-none-any.whl\n",
            "Collecting et_xmlfile (from openpyxl)\n",
            "  Downloading https://files.pythonhosted.org/packages/22/28/a99c42aea746e18382ad9fb36f64c1c1f04216f41797f2f0fa567da11388/et_xmlfile-1.0.1.tar.gz\n",
            "Building wheels for collected packages: openpyxl, et-xmlfile\n",
            "  Running setup.py bdist_wheel for openpyxl ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b8/20/5b/d260e131180f4394eba48d97e238016f4bde050727fce79283\n",
            "  Running setup.py bdist_wheel for et-xmlfile ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2a/77/35/0da0965a057698121fc7d8c5a7a9955cdbfb3cc4e2423cad39\n",
            "Successfully built openpyxl et-xmlfile\n",
            "Installing collected packages: jdcal, et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.0.1 jdcal-1.4 openpyxl-2.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "do-YJoP1Ue6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nS_oeFjBc3mR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwebuPryc3mX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#scope=['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
        "#credentials = ServiceAccountCredentials.from_json_keyfile_name('DDDproject-f724d034f3f7.json', scope)\n",
        "#gc = gspread.authorize(credentials)\n",
        "\n",
        "\n",
        "#google colab autentication\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eH55oVXHc3mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input parameters for data\n",
        "\n",
        "#google sheet needs to be shared with the account gsheet-auctoritates@dddproject-1086.iam.gserviceaccount.com\n",
        "source_abbreviation_url = \"https://docs.google.com/spreadsheets/d/1byVbRwnQ058--0ud1_NBt2xgIhuiV7LK1BmCm67CTxA/edit#gid=1657192709\"#url tabulky\n",
        "source_abbreviation_url_sheet = \"AbbrevConverter\" #nazev konkretniho sheetu\n",
        "\n",
        "source_datasets_url = [[\"https://docs.google.com/spreadsheets/d/1byVbRwnQ058--0ud1_NBt2xgIhuiV7LK1BmCm67CTxA/edit#gid=1657192709\",\"KatpramBiblicalQuotations\"]]  # muze jich byt vic, budeme concatovat\n",
        "\n",
        "projectName = \"AUCT-\"\n",
        "for dt in source_datasets_url:\n",
        "    projectName += dt[1]\n",
        "\n",
        "\n",
        "#getting abbreviation sheet\n",
        "wksabb = gc.open_by_url(source_abbreviation_url).worksheet(source_abbreviation_url_sheet)\n",
        "\n",
        "#gettting data sheets\n",
        "df_references = []\n",
        "for source in source_datasets_url:\n",
        "    wksdata = gc.open_by_url(source[0]).worksheet(source[1])\n",
        "    df_references.append(pd.DataFrame(wksdata.get_all_records()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z4aLlnEmc3mh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input parameters for operations\n",
        "\n",
        "#reference string in references dataset\n",
        "itemReferenceColumn = \"Full_reference_Czech\"\n",
        "itemReferenceDenotation = itemReferenceColumn.split(\"_\")[-1] #Czech\n",
        "\n",
        "\n",
        "#params for understanding of reference parts NOT USED\n",
        "signVerseBook = ','\n",
        "signFromTo = '-'\n",
        "signAnd = '.'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IVmFHJb0c3mn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#params for output generation  partially USED\n",
        "\n",
        "import time\n",
        "ts = time.gmtime()\n",
        "tstring = time.strftime(\"%Y-%m-%d-%H-%M-%S\", ts)\n",
        "# 2018-08-05-22-20-39\n",
        "outputfilename = \"output_\" + projectName + \"_\" + tstring\n",
        "\n",
        "classificationLevel1 = \"název sloupce\" # [hodnoty = Cathar, catholic] [řazení vždy abecedně-číselné, ale lze zadat parametr variantního sloupce, podle kterého sortovat]\n",
        "classificationLevel2 = \"název sloupce\" # [hodnoty = OT/NT]\n",
        "#...\n",
        "classificationLevel6 = \"název sloupce\"\n",
        "\n",
        "\n",
        "\n",
        "#Prototyp (Pokřtění ohněm):\n",
        "#classificationLevel1 = [\"Christianity_type_level1\",\"\"]  #cathar/catholic\n",
        "#classificationLevel2 = [\"Classification_testament\",sortBy=[\"OT\",\"NT\"]] #NT/OT\n",
        "#classificationLevel3 = [\"Czech_full\", sortBy=Book_ID]\n",
        "\n",
        "\n",
        "#Prototyp (Pokřtění ohněm):  USED\n",
        "classificationLevel1 = [\"Christianity_type_level1\",\"\"]  #cathar/catholic\n",
        "classificationLevel2 = [\"Classification_testament\",[\"OT\",\"NT\"]] #NT/OT\n",
        "classificationLevel3 = [\"Czech_full\", \"Book_ID\"]\n",
        "\n",
        "\n",
        "#NOT USED\n",
        "rejoinSeparatedItems = True #[default=False] [“+” a “=” => u každého rozděleného citátu musí být do velké rozsekané tabulky vepsáno a) že to je rozdělená reference b) její typ (typ “+” nebo typ “=”)]\n",
        "\n",
        "#NOT USED\n",
        "distinguishCitationAllusion = False # [default=True]\n",
        "\n",
        "\n",
        "#USED\n",
        "leftDelimiterAllusion= '(' # [Určení, jak má v kódu HTML vypadat levý vymezovač aluze]\n",
        "rightDelimiterAllusion= ')'\n",
        "leftDelimiterCitation= ''\n",
        "leftDelimiterCitation= ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-3WorO_c3ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cleaning dataset from empty rows/cells\n",
        "\n",
        "df_abb = pd.DataFrame(wksabb.get_all_records()) #abbreviation dataset\n",
        "df_abb_signatures = df_abb[df_abb[\"Book_ID\"]  == \"N/A\"] #rows with punctiation symbols\n",
        "\n",
        "\n",
        "#replace empty strings with \"NaN\"\n",
        "df_abb = df_abb.applymap(lambda x: np.nan if isinstance(x, basestring) and (len(x) == 0 or x==\"N/A\") else x)\n",
        "#remove rows which do not have set the type of testament (OT/NT)\n",
        "df_abb = df_abb.dropna(subset=['Classification_testament'])\n",
        "\n",
        "\n",
        "for idx, df in enumerate(df_references):\n",
        "    #replace empty strings with \"NaN\"\n",
        "    df = df.applymap(lambda x: np.nan if isinstance(x, basestring) and len(x) == 0 else x)\n",
        "    #remove rows which do not have set the type of citations (ciation/allusion)\n",
        "    df = df.dropna(subset=['Citation_or_allusion'])\n",
        "    df_references[idx] = df\n",
        "    \n",
        "#df_abb_signatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_d2hbIfc3my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df_references[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVNGPPdJc3m2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#enrichment of references dataset\n",
        "#rtype >  single / multiple / mitem\n",
        "\n",
        "\n",
        "def is_reference_multiple(teststring):\n",
        "    possibles = [\"+\",\"=\",\";\"]    \n",
        "    \n",
        "    if any(s in teststring for s in possibles):  #multiple reference type\n",
        "        for sign in possibles:\n",
        "            if (sign in teststring):\n",
        "                found = sign\n",
        "                \n",
        "        return \"multiple\"+found\n",
        "    else:\n",
        "        return \"single\"  \n",
        "\n",
        "def count_children_reference_multiple(row):        \n",
        "    teststring = row[itemReferenceColumn]\n",
        "    rowid = row.index\n",
        "    possibles = [\"+\",\"=\",\";\"]    \n",
        "    \n",
        "    if any(s in teststring for s in possibles):  #multiple reference type        \n",
        "        for sign in possibles:\n",
        "            if (sign in teststring):\n",
        "                parts = teststring.split(sign)            \n",
        "        count = len(parts)        \n",
        "        return count\n",
        "    else:\n",
        "        return 1      \n",
        "\n",
        "   \n",
        "    \n",
        "\n",
        "#new columns for composition manipulation\n",
        "for idx, df in enumerate(df_references):\n",
        "    df['Composite_type'] = df.apply(lambda row: is_reference_multiple(row[itemReferenceColumn]), axis=1)\n",
        "    df['Composite_count'] = df.apply(lambda row: count_children_reference_multiple(row), axis=1)\n",
        "    df['Composite_parent'] = df.index\n",
        "    df['Composite_reference'] = df[itemReferenceColumn]\n",
        "    \n",
        "    #iterate over dataset and create children for multi items\n",
        "    for index, row in df.iterrows():\n",
        "        if (row['Composite_count'] > 1):\n",
        "                                  \n",
        "            parts = row[itemReferenceColumn].split(row['Composite_type'][-1])\n",
        "            \n",
        "            row['Composite_type'] = 'child'+ row['Composite_type'][-1]\n",
        "            row['Composite_parent'] = index\n",
        "            row['Composite_reference'] = \"\"\n",
        "            \n",
        "            for part in parts:\n",
        "                row['Composite_reference'] = part.strip()           \n",
        "                df.loc[len(df.index)]= row  #create child row\n",
        "    \n",
        "    \n",
        "    df_references[idx] = df\n",
        "    \n",
        "#df_references[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CInGKDQc3m6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03d45f73-96d2-4598-c002-a858e0c85afd"
      },
      "cell_type": "code",
      "source": [
        "#enrichment of references dataset\n",
        "\n",
        "\n",
        "#book_indentity > determination of bible book for the reference\n",
        "def determine_book_abb(row):\n",
        "    \n",
        "    if (\"multiple\" not in row['Composite_type']):\n",
        "        \n",
        "        string = row['Composite_reference'].split(\" \")\n",
        "        \n",
        "        if (string[0].isdigit()): \n",
        "            book_abb = string[0] + \" \" + string[1]\n",
        "        else:\n",
        "            book_abb = string[0]        \n",
        "        \n",
        "        return book_abb\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "#chapter determination    \n",
        "def determine_chapter(row):\n",
        "    \n",
        "    if (\"multiple\" not in row['Composite_type']):\n",
        "        \n",
        "        string = row['Composite_reference'].split(\" \")\n",
        "        \n",
        "        if (string[0].isdigit()): \n",
        "            book_abb = string[0] + \" \" + string[1]\n",
        "        else:\n",
        "            book_abb = string[0]        \n",
        "        \n",
        "        lookingforchapter = row['Composite_reference'].split(\",\")\n",
        "        \n",
        "        if (len(lookingforchapter) > 2):\n",
        "            #raise ValueError('More chapter delimiters in composite reference: ' + row['Composite_reference'])\n",
        "            print 'Warning','More chapter delimiters in composite reference: ',row['Composite_reference']\n",
        "        \n",
        "        chapter = lookingforchapter[0].replace(book_abb,\"\")\n",
        "        \n",
        "        return int(chapter)\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "#verses determinantion      \n",
        "def determine_verses(row):\n",
        "    \n",
        "    if (\"multiple\" not in row['Composite_type']):\n",
        "        \n",
        "        string = row['Composite_reference'].split(\" \")\n",
        "        \n",
        "        if (string[0].isdigit()): \n",
        "            book_abb = string[0] + \" \" + string[1]\n",
        "        else:\n",
        "            book_abb = string[0]        \n",
        "        \n",
        "        lookingforverses = row['Composite_reference'].split(\",\")\n",
        "        \n",
        "        if (len(lookingforverses) > 2):\n",
        "            #raise ValueError('A more chapter delimiters in composite reference: ' + row['Composite_reference'])\n",
        "            print 'Warning','A more chapter delimiters in composite reference: ',row['Composite_reference']\n",
        "        \n",
        "        verses = lookingforverses[1]\n",
        "        \n",
        "        return verses\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "#first verse determinantion \n",
        "def determine_firstverse(row):\n",
        "    \n",
        "    if (\"multiple\" not in row['Composite_type']):\n",
        "        verses = row['Composite_reference_verses']\n",
        "        if (\" \" in verses):\n",
        "            verses = verses.split(\" \")[0]\n",
        "    \n",
        "        if (\"-\" in verses):\n",
        "            string = verses.split(\"-\") \n",
        "            verse = string[0]\n",
        "            \n",
        "            if (\".\" in verse):\n",
        "                string = verse.split(\".\")\n",
        "                if (len(string) > 2):\n",
        "                    verse = string[0] + \".\" + string[1]\n",
        "                else:\n",
        "                    verse = string[0]\n",
        "    \n",
        "            return float(verse)\n",
        "        else:\n",
        "            \n",
        "            if (\".\" in verses):\n",
        "                string = verses.split(\".\")\n",
        "                if (len(string) > 2):\n",
        "                    verses = string[0] + \".\" + string[1]\n",
        "                else:\n",
        "                    verses = string[0]\n",
        "            \n",
        "            return float(verses)    \n",
        "    else:\n",
        "        return np.nan    \n",
        "\n",
        "#count duplicate references,  result number is the count of item in the classification level1\n",
        "def determine_aggregates(row,df,classificationLevel1):    \n",
        "    target = row['Composite_reference']    \n",
        "    sames = df.loc[(df['Composite_reference'] == target) & (df[classificationLevel1[0]] == row[classificationLevel1[0]]) & (df['Citation_or_allusion'] == row['Citation_or_allusion'])]    \n",
        "    aggcount = len(sames)    \n",
        "    return aggcount\n",
        "\n",
        "#aggregation of sources for duplicate references\n",
        "def determine_sources(row,df,classificationLevel1):   \n",
        "    target = row['Composite_reference']\n",
        "    sources_string = unicode(row['Work']) +\"/\"+ unicode(row['Work_part'])\n",
        "    \n",
        "    if (row['Aggregation'] > 1):\n",
        "        source_string = ''\n",
        "        sources = []\n",
        "        sames = df.loc[(df['Composite_reference'] == target) & (df[classificationLevel1[0]] == row[classificationLevel1[0]]) & (df['Citation_or_allusion'] == row['Citation_or_allusion'])]\n",
        "        sames = sames.sort_values([\"ID\",\"Work\",\"Work_part\"]) \n",
        "        for index, row in sames.iterrows():\n",
        "            sources.append(unicode(row['Work']) +\"/\"+ unicode(row['Work_part']))\n",
        "        sources_string = ', '.join(sources) \n",
        "        \n",
        "        #make one of aggregates unique\n",
        "        i = 0\n",
        "        for index, row in sames.iterrows():\n",
        "            if (i == 0):\n",
        "                df.at[index, 'Aggregation'] = row['Aggregation']\n",
        "            else:\n",
        "                df.at[index, 'Aggregation'] = -1\n",
        "            i += 1\n",
        "                \n",
        "    return sources_string\n",
        "\n",
        "#new columns in dataset\n",
        "for idx, df in enumerate(df_references):\n",
        "    df['Book_abb'] = df.apply(lambda row: determine_book_abb(row), axis=1)\n",
        "    df['Composite_reference_chapter'] = df.apply(lambda row: determine_chapter(row), axis=1)\n",
        "    df['Composite_reference_verses'] = df.apply(lambda row: determine_verses(row), axis=1)\n",
        "    df['Composite_reference_firstverse'] = df.apply(lambda row: determine_firstverse(row), axis=1)\n",
        "    df['Aggregation'] = df.apply(lambda row: determine_aggregates(row,df,classificationLevel1), axis=1)      \n",
        "    df['Reference_sources'] = df.apply(lambda row: determine_sources(row,df,classificationLevel1), axis=1)\n",
        "\n",
        "#df_references[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning More chapter delimiters in composite reference:  1 K 12,31.13,1-3\n",
            "Warning A more chapter delimiters in composite reference:  1 K 12,31.13,1-3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "383AUVBKc3nC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#merge of references with abbreviation on bible book\n",
        "\n",
        "for i in range(0,len(df_references)):\n",
        "    df_references[i] = df_references[i].merge(df_abb,left_on='Book_abb', right_on='Czech', how='outer')\n",
        "    #print df_references[i].head(1)\n",
        "\n",
        "    \n",
        "#merge if multipledataset\n",
        "if (len(df_references) > 1):\n",
        "    dff = pd.concat(df_references, join=\"inner\")\n",
        "else:\n",
        "    dff = df_references[0]\n",
        "\n",
        "#print dff.head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TxxBN2gJc3nI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test output to excel file \n",
        "\n",
        "dff = dff.dropna(subset=[itemReferenceColumn])\n",
        "dff.sort_values([classificationLevel1[0],classificationLevel2[0],\"Book_ID\",\"Composite_reference_chapter\",\"Composite_reference_firstverse\"])\n",
        "dff.to_excel(outputfilename+ \".xlsx\")\n",
        "files.download(outputfilename+ \".xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbRsYfZgc3nM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#html template\n",
        "\n",
        "from string import Template\n",
        "\n",
        "#OT/NT LEVEL\n",
        "\n",
        "basic_frame = \"\"\"\n",
        "<html>\n",
        "<head><meta charset=\"UTF-8\"><title>Output test for Auctoritates Prototype ($outputfilename)</title>\n",
        "<style>\n",
        "span {display:block; }\n",
        "span book {font-style:italic}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "$first_frame\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "t0 = Template(basic_frame)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vNrjJ-ac3nW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#generate html output\n",
        "\n",
        "#It produces indices of biblical quotes structured by biblical book\n",
        "#(v pořadí určeném určeným číselnou hodnotou ve sloupci “ID” tohoto listu). \n",
        "#Under the book, the quotes are ordered by chapter and verse.\n",
        "\n",
        "\n",
        "classifiers1 = dff[classificationLevel1[0]].unique().tolist()\n",
        "classifiers2 = dff[classificationLevel2[0]].unique().tolist()\n",
        "\n",
        "\n",
        "output = ''\n",
        "\n",
        "for classifier1 in classifiers1:   \n",
        "    if (classifier1 == np.nan):\n",
        "        continue\n",
        "    \n",
        "    output += \"<h2>\" + unicode(classifier1) + \"</h2><p>\"         \n",
        "    \n",
        "    for classifier2 in classifiers2:\n",
        "        \n",
        "        if (str(classifier2) == \"nan\"):\n",
        "            continue\n",
        "                \n",
        "        output += \"</p><h3>\" + classifier2 + \"</h3><p>\"\n",
        "        \n",
        "        \n",
        "        #print \"Comparing\",classificationLevel1[0],classifier1,' a ',classificationLevel2[0],classifier2\n",
        "        class_data = dff.loc[(dff[classificationLevel1[0]]==classifier1) & (dff[classificationLevel2[0]]==classifier2)]\n",
        "        \n",
        "        class_data = class_data.sort_values([\"Book_ID\",\"Composite_reference_chapter\",\"Composite_reference_firstverse\"])\n",
        "        \n",
        "        section = ''\n",
        "        section_last =''\n",
        "        \n",
        "        for index, row in class_data.iterrows():\n",
        "            reference = ''\n",
        "            \n",
        "            if (unicode(row['Czech_full']) == \"nan\"): # pass multiple and composite references [they are included in children]\n",
        "                #print \"Warning: beacuse of empty Czech_full passing ID: \" + unicode(row['ID'])\n",
        "                #print row\n",
        "                continue\n",
        "  \n",
        "            if (row['Aggregation'] == -1): # pass references which are part of aggregation\n",
        "                continue        \n",
        "                \n",
        "            if (row['Citation_or_allusion'] == \"a\"):\n",
        "                before = \"<span class='a'>\"+leftDelimiterAllusion\n",
        "                after = rightDelimiterAllusion\n",
        "            else:\n",
        "                before = \"<span class='c'>\"+leftDelimiterCitation\n",
        "                after = leftDelimiterCitation\n",
        "                \n",
        "            \n",
        "            section = row['Czech_full']\n",
        "            \n",
        "            if (section_last != section):              \n",
        "                \n",
        "                output += \"</p><h4>\" +section + \"</h4><p>\"\n",
        "            \n",
        "            reference += before            \n",
        "                        \n",
        "            if (\"multiple\" not in row['Composite_type']):\n",
        "                reference +=  unicode(row['Composite_reference'])   \n",
        "                \n",
        "            if ((\"child\" in row['Composite_type']) & (row['Composite_type'][-1] != \";\")):                \n",
        "                full = unicode(row['Full_reference_Czech'])\n",
        "                fullparts = full.split(row['Composite_type'][-1])\n",
        "                fullparts = [x.strip(' ') for x in fullparts]                \n",
        "                #print row['Composite_type'][-1],fullparts, row['Composite_reference']\n",
        "                fullparts.remove(row['Composite_reference'])                \n",
        "                \n",
        "                #to get cursive around books abb in multireference output\n",
        "                partem = []\n",
        "                for part in fullparts:\n",
        "                    div = part.split(\" \")\n",
        "                    if (len(div)>3):\n",
        "                        print \"Warning: Strange multipart \" + div                    \n",
        "                    if (len(div)>2):\n",
        "                        partem.append(\"<i>\"+ div[0] +\" \"+ div[1]+\"</i>\" + div[2])\n",
        "                    else:\n",
        "                        partem.append(\"<i>\"+ div[0] +\"</i> \"+ div[1]) \n",
        "                \n",
        "                \n",
        "                full = row['Composite_type'][-1]+ \" \" + (\" \" + row['Composite_type'][-1]+\" \").join(partem)                    \n",
        "                reference += \" [\"+ full +\"]\"\n",
        "                \n",
        "            reference = reference.replace(row['Book_abb'],\"<i>\"+unicode(row['Book_abb'])+\"</i>\")            \n",
        "            reference += after\n",
        "            reference += \" .... \" + unicode(row['Reference_sources']) + \"\\n\"                    \n",
        "            \n",
        "            reference += \"</span>\"\n",
        "            \n",
        "            output += reference            \n",
        "            section_last = row['Czech_full']\n",
        "\n",
        "#print output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJR5cgWBc3nZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#d3 = {'h3_section_title':'h3 title', 'content_frame':\"content\"}\n",
        "#html3 = t3.substitute(d3)\n",
        "#d2 = {'h2_section_title':'h2 title', 'third_frame':html3}\n",
        "#html2 = t2.substitute(d2)\n",
        "#d1 = {'h1_section_title':'h1 title', 'second_frame':html2}\n",
        "#html1 =t1.substitute(d1)\n",
        "#d0 = {'first_frame':html1}\n",
        "#html = t0.substitute(d0)\n",
        "\n",
        "html = t0.substitute(first_frame=output,outputfilename=outputfilename)\n",
        "\n",
        "#Html_file= open(outputfile + \".html\",\"w\")\n",
        "#Html_file.write(html.encode(\"UTF-8\"))\n",
        "#Html_file.close()\n",
        "\n",
        "#generate file in colab, will prompt browser download dialog\n",
        "with open(outputfilename + \".html\", 'w') as f:\n",
        "  f.write(html.encode(\"UTF-8\"))\n",
        "files.download(outputfilename + \".html\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HXxCPSQbc3nf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bhYa2W4Fc3nk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MlHTmu1jc3no",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "35972f51-68c2-4754-badc-ba6a6f566585"
      },
      "cell_type": "code",
      "source": [
        "#NOT USED\n",
        "\n",
        "#generate  output TEXTUAL\n",
        "\n",
        "#It produces indices of biblical quotes structured by biblical book\n",
        "#(v pořadí určeném určeným číselnou hodnotou ve sloupci “ID” tohoto listu). \n",
        "#Under the book, the quotes are ordered by chapter and verse.\n",
        "\n",
        "\"\"\"\n",
        "classifiers1 = dff[classificationLevel1[0]].unique().tolist()\n",
        "classifiers2 = dff[classificationLevel2[0]].unique().tolist()\n",
        "\n",
        "\n",
        "output = ''\n",
        "\n",
        "for classifier1 in classifiers1:   \n",
        "    if (classifier1 == np.nan):\n",
        "        continue\n",
        "    \n",
        "    output += \"\\n\"+ \"\\n\" + classifier1 + \"\\n\"     \n",
        "    output += \"******************************************\"\n",
        "    \n",
        "    for classifier2 in classifiers2:\n",
        "        \n",
        "        if (str(classifier2) == \"nan\"):\n",
        "            continue\n",
        "                \n",
        "        output += \"\\n\" +\"\\n\" + classifier2 + \"\\n\"\n",
        "        output += \"-----------------------------------------\" + \"\\n\"\n",
        "        \n",
        "        #print \"Comparing\",classificationLevel1[0],classifier1,' a ',classificationLevel2[0],classifier2\n",
        "        class_data = dff.loc[(dff[classificationLevel1[0]]==classifier1) & (dff[classificationLevel2[0]]==classifier2)]\n",
        "        \n",
        "        class_data = class_data.sort_values(\"Book_ID\")\n",
        "        \n",
        "        section = ''\n",
        "        section_last =''\n",
        "        \n",
        "        for index, row in class_data.iterrows():\n",
        "            reference = ''\n",
        "            \n",
        "            if (unicode(row['Czech_full']) == \"nan\"):\n",
        "                continue\n",
        "                \n",
        "            if (row['Citation_or_allusion'] == \"a\"):\n",
        "                before = leftDelimiterAllusion\n",
        "                after = rightDelimiterAllusion\n",
        "            else:\n",
        "                before = leftDelimiterCitation\n",
        "                after = leftDelimiterCitation\n",
        "                \n",
        "            \n",
        "            section = row['Czech_full']\n",
        "            \n",
        "            if (section_last != section):              \n",
        "                \n",
        "                output += \"\\n\" +section + \"\\n\"\n",
        "            \n",
        "            reference += before            \n",
        "                        \n",
        "            if (\"multiple\" not in row['Composite_type']):\n",
        "                reference +=  unicode(row['Composite_reference'])\n",
        "                \n",
        "            if (\"child\" in row['Composite_type']):                \n",
        "                full = unicode(row['Full_reference_Czech'])\n",
        "                fullparts = full.split(row['Composite_type'][-1])\n",
        "                fullparts = [x.strip(' ') for x in fullparts]                \n",
        "                #print row['Composite_type'][-1],fullparts, row['Composite_reference']\n",
        "                fullparts.remove(row['Composite_reference'])                \n",
        "                full = row['Composite_type'][-1]+ \" \" + (\" \" + row['Composite_type'][-1]+\" \").join(fullparts)                    \n",
        "                reference += \"[\"+ full +\"]\"\n",
        "                        \n",
        "            reference += after                              \n",
        "            output += reference            \n",
        "            section_last = row['Czech_full']\n",
        "\n",
        "print output\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclassifiers1 = dff[classificationLevel1[0]].unique().tolist()\\nclassifiers2 = dff[classificationLevel2[0]].unique().tolist()\\n\\n\\noutput = \\'\\'\\n\\nfor classifier1 in classifiers1:   \\n    if (classifier1 == np.nan):\\n        continue\\n    \\n    output += \"\\n\"+ \"\\n\" + classifier1 + \"\\n\"     \\n    output += \"******************************************\"\\n    \\n    for classifier2 in classifiers2:\\n        \\n        if (str(classifier2) == \"nan\"):\\n            continue\\n                \\n        output += \"\\n\" +\"\\n\" + classifier2 + \"\\n\"\\n        output += \"-----------------------------------------\" + \"\\n\"\\n        \\n        #print \"Comparing\",classificationLevel1[0],classifier1,\\' a \\',classificationLevel2[0],classifier2\\n        class_data = dff.loc[(dff[classificationLevel1[0]]==classifier1) & (dff[classificationLevel2[0]]==classifier2)]\\n        \\n        class_data = class_data.sort_values(\"Book_ID\")\\n        \\n        section = \\'\\'\\n        section_last =\\'\\'\\n        \\n        for index, row in class_data.iterrows():\\n            reference = \\'\\'\\n            \\n            if (unicode(row[\\'Czech_full\\']) == \"nan\"):\\n                continue\\n                \\n            if (row[\\'Citation_or_allusion\\'] == \"a\"):\\n                before = leftDelimiterAllusion\\n                after = rightDelimiterAllusion\\n            else:\\n                before = leftDelimiterCitation\\n                after = leftDelimiterCitation\\n                \\n            \\n            section = row[\\'Czech_full\\']\\n            \\n            if (section_last != section):              \\n                \\n                output += \"\\n\" +section + \"\\n\"\\n            \\n            reference += before            \\n                        \\n            if (\"multiple\" not in row[\\'Composite_type\\']):\\n                reference +=  unicode(row[\\'Composite_reference\\'])\\n                \\n            if (\"child\" in row[\\'Composite_type\\']):                \\n                full = unicode(row[\\'Full_reference_Czech\\'])\\n                fullparts = full.split(row[\\'Composite_type\\'][-1])\\n                fullparts = [x.strip(\\' \\') for x in fullparts]                \\n                #print row[\\'Composite_type\\'][-1],fullparts, row[\\'Composite_reference\\']\\n                fullparts.remove(row[\\'Composite_reference\\'])                \\n                full = row[\\'Composite_type\\'][-1]+ \" \" + (\" \" + row[\\'Composite_type\\'][-1]+\" \").join(fullparts)                    \\n                reference += \"[\"+ full +\"]\"\\n                        \\n            reference += after                              \\n            output += reference            \\n            section_last = row[\\'Czech_full\\']\\n\\nprint output\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}